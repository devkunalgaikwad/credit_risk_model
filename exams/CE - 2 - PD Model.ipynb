{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data and Selecting the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv('fe_2_loan_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_train = loan_data.loc[loan_data['test'] == 0, : ].drop(['good_bad', 'test'], axis = 1)\n",
    "loan_data_targets_train = loan_data['good_bad'][loan_data['test'] == 0]\n",
    "loan_data_inputs_test = loan_data.loc[loan_data['test'] == 1, : ].drop(['good_bad', 'test'], axis = 1)\n",
    "loan_data_targets_test = loan_data['good_bad'][loan_data['test'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the names of the reference category dummy variables in a list.\n",
    "ref_categories = ['grade:G',\n",
    "'home_ownership:RENT',\n",
    "'verification_status:Source Verified',\n",
    "'purpose:sm_b__educ__mov',\n",
    "'term:60',\n",
    "'emp_length:0',\n",
    "'mths_since_issue_d:>77',\n",
    "'int_rate:>20.281',\n",
    "'mths_since_earliest_cr_line:<140',\n",
    "'acc_now_delinq:>=1',\n",
    "'inq_last_6mths:>=3',\n",
    "'annual_inc:<40K',\n",
    "'dti:21.7-22.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs_train = loan_data_inputs_train.drop(ref_categories, axis = 1)\n",
    "# From the dataframe with input variables, we drop the variables with variable names in the list with reference categories. \n",
    "inputs_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression()\n",
    "# We create an instance of an object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "# Sets the pandas dataframe options to display all columns/ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.fit(inputs_train, loan_data_targets_train)\n",
    "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
    "# with inputs (independent variables) contained in the first dataframe\n",
    "# and targets (dependent variables) contained in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.intercept_\n",
    "# Displays the intercept contain in the estimated (\"fitted\") object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.coef_\n",
    "# Displays the coefficients contained in the estimated (\"fitted\") object from the 'LogisticRegression' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = inputs_train.columns.values\n",
    "# Stores the names of the columns of a dataframe in a variable.\n",
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
    "summary_table['Coefficients'] = np.transpose(reg.coef_)\n",
    "# Creates a new column in the dataframe, called 'Coefficients',\n",
    "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
    "summary_table.index = summary_table.index + 1\n",
    "# Increases the index of every row of the dataframe with 1.\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "# Assigns values of the row with index 0 of the dataframe.\n",
    "summary_table = summary_table.sort_index()\n",
    "# Sorts the dataframe by index.\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Logistic Regression Model with P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P values for sklearn logistic regression.\n",
    "\n",
    "# Class to display p-values for logistic regression in sklearn.\n",
    "\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticRegression_with_p_values:\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        \n",
    "        #### Get p-values for the fitted model ####\n",
    "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X / denom).T,X) ## Fisher Information Matrix\n",
    "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0] / sigma_estimates # z-score for eaach model coefficient\n",
    "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n",
    "        \n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        #self.z_scores = z_scores\n",
    "        self.p_values = p_values\n",
    "        #self.sigma_estimates = sigma_estimates\n",
    "        #self.F_ij = F_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticRegression_with_p_values:\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X / denom).T,X)\n",
    "        Cramer_Rao = np.linalg.inv(F_ij)\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0] / sigma_estimates\n",
    "        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores]\n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        self.p_values = p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression_with_p_values()\n",
    "# We create an instance of an object from the newly created 'LogisticRegression_with_p_values()' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg.fit(inputs_train, loan_data_targets_train)\n",
    "# Estimates the coefficients of the object from the 'LogisticRegression' class\n",
    "# with inputs (independent variables) contained in the first dataframe\n",
    "# and targets (dependent variables) contained in the second dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = inputs_train.columns.values\n",
    "# Stores the names of the columns of a dataframe in a variable.\n",
    "summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n",
    "# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\n",
    "summary_table['Coefficient'] = np.transpose(reg.coef_)\n",
    "# Creates a new column in the dataframe, called 'Coefficients',\n",
    "# with row values the transposed coefficients from the 'LogisticRegression' object.\n",
    "summary_table.index = summary_table.index + 1\n",
    "# Increases the index of every row of the dataframe with 1.\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "# Assigns values of the row with index 0 of the dataframe.\n",
    "summary_table = summary_table.sort_index()\n",
    "# Sorts the dataframe by index.\n",
    "p_values = reg.p_values\n",
    "# This is a list. We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\n",
    "p_values = np.append(np.nan,np.array(p_values))\n",
    "# Add the intercept. We add the value 'NaN' in the beginning of the variable with p-values.\n",
    "summary_table['p-value'] = p_values\n",
    "# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Model Validation (Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample validation (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = loan_data_inputs_test.drop(ref_categories, axis = 1)\n",
    "inputs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = reg.model.predict(inputs_test)\n",
    "# Calculates the predicted values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test\n",
    "# This is an array of predicted discrete classess (in this case, 0s and 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba = reg.model.predict_proba(inputs_test)\n",
    "# Calculates the predicted probability values for the dependent variable (targets)\n",
    "# based on the values of the independent variables (inputs) supplied as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba\n",
    "# This is an array of arrays of predicted class probabilities for all classes.\n",
    "# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n",
    "# and the second value is the probability for the observation to belong to the first class, i.e. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba[:][:,1]\n",
    "# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n",
    "# that is, the second element.\n",
    "# In other words, we take only the probabilities for being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba = y_hat_test_proba[: ][: , 1]\n",
    "# We store these probabilities in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba\n",
    "# This variable contains an array of probabilities of being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test_temp = loan_data_targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_targets_test_temp.reset_index(drop = True, inplace = True)\n",
    "# We reset the index of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = pd.concat([loan_data_targets_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)\n",
    "# Concatenates two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.columns = ['loan_data_targets_test', 'y_hat_test_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.index = loan_data_inputs_test.index\n",
    "# Makes the index of one dataframe equal to the index of another dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Area under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 0.8\n",
    "# We create a new column with an indicator,\n",
    "# where every observation that has predicted probability greater than the threshold has a value of 1,\n",
    "# and every observation that has predicted probability lower than the threshold has a value of 0.\n",
    "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])\n",
    "# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n",
    "# This table is known as a Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]\n",
    "# Here we divide each value of the table by the total number of observations,\n",
    "# thus getting percentages, or, rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]\n",
    "# Here we calculate Accuracy of the model, which is the sum of the diagonal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n",
    "# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Here we store each of the three arrays in a separate variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n",
    "# thus plotting the ROC curve.\n",
    "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "# We plot a seconary diagonal line, with dashed line style and black color.\n",
    "plt.xlabel('False positive rate')\n",
    "# We name the x-axis \"False positive rate\".\n",
    "plt.ylabel('True positive rate')\n",
    "# We name the x-axis \"True positive rate\".\n",
    "plt.title('ROC curve')\n",
    "# We name the graph \"ROC curve\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC = roc_auc_score(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])\n",
    "# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
    "# from a set of actual values and their predicted probabilities.\n",
    "AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini and Kolmogorov-Smirnov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = df_actual_predicted_probs.sort_values('y_hat_test_proba')\n",
    "# Sorts a dataframe by the values of a specific column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs = df_actual_predicted_probs.reset_index()\n",
    "# We reset the index of a dataframe and overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['Cumulative N Population'] = df_actual_predicted_probs.index + 1\n",
    "# We calculate the cumulative number of all observations.\n",
    "# We use the new index for that. Since indexing in ython starts from 0, we add 1 to each index.\n",
    "df_actual_predicted_probs['Cumulative N Good'] = df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
    "# We calculate cumulative number of 'good', which is the cumulative sum of the column with actual observations.\n",
    "df_actual_predicted_probs['Cumulative N Bad'] = df_actual_predicted_probs['Cumulative N Population'] - df_actual_predicted_probs['loan_data_targets_test'].cumsum()\n",
    "# We calculate cumulative number of 'bad', which is\n",
    "# the difference between the cumulative number of all observations and cumulative number of 'good' for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['Cumulative Perc Population'] = df_actual_predicted_probs['Cumulative N Population'] / (df_actual_predicted_probs.shape[0])\n",
    "# We calculate the cumulative percentage of all observations.\n",
    "df_actual_predicted_probs['Cumulative Perc Good'] = df_actual_predicted_probs['Cumulative N Good'] / df_actual_predicted_probs['loan_data_targets_test'].sum()\n",
    "# We calculate cumulative percentage of 'good'.\n",
    "df_actual_predicted_probs['Cumulative Perc Bad'] = df_actual_predicted_probs['Cumulative N Bad'] / (df_actual_predicted_probs.shape[0] - df_actual_predicted_probs['loan_data_targets_test'].sum())\n",
    "# We calculate the cumulative percentage of 'bad'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Gini\n",
    "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Bad'])\n",
    "# We plot the cumulative percentage of all along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
    "# thus plotting the Gini curve.\n",
    "plt.plot(df_actual_predicted_probs['Cumulative Perc Population'], df_actual_predicted_probs['Cumulative Perc Population'], linestyle = '--', color = 'k')\n",
    "# We plot a seconary diagonal line, with dashed line style and black color.\n",
    "plt.xlabel('Cumulative % Population')\n",
    "# We name the x-axis \"Cumulative % Population\".\n",
    "plt.ylabel('Cumulative % Bad')\n",
    "# We name the y-axis \"Cumulative % Bad\".\n",
    "plt.title('Gini')\n",
    "# We name the graph \"Gini\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini = AUROC * 2 - 1\n",
    "# Here we calculate Gini from AUROC.\n",
    "Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.72 * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KS\n",
    "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Bad'], color = 'r')\n",
    "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'bad' along the y-axis,\n",
    "# colored in red.\n",
    "plt.plot(df_actual_predicted_probs['y_hat_test_proba'], df_actual_predicted_probs['Cumulative Perc Good'], color = 'b')\n",
    "# We plot the predicted (estimated) probabilities along the x-axis and the cumulative percentage 'good' along the y-axis,\n",
    "# colored in red.\n",
    "plt.xlabel('Estimated Probability for being Good')\n",
    "# We name the x-axis \"Estimated Probability for being Good\".\n",
    "plt.ylabel('Cumulative %')\n",
    "# We name the y-axis \"Cumulative %\".\n",
    "plt.title('Kolmogorov-Smirnov')\n",
    "# We name the graph \"Kolmogorov-Smirnov\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KS = max(df_actual_predicted_probs['Cumulative Perc Bad'] - df_actual_predicted_probs['Cumulative Perc Good'])\n",
    "# We calculate KS from the data. It is the maximum of the difference between the cumulative percentage of 'bad'\n",
    "# and the cumulative percentage of 'good'.\n",
    "KS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the PD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "# Sets the pandas dataframe options to display all columns/ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])\n",
    "# We create a new dataframe with one column. Its values are the values from the 'reference_categories' list.\n",
    "# We name it 'Feature name'.\n",
    "df_ref_categories['Coefficient'] = 0\n",
    "# We create a second column, called 'Coefficients', which contains only 0 values.\n",
    "df_ref_categories['p-value'] = np.nan\n",
    "# We create a third column, called 'p_values', with contains only NaN values.\n",
    "df_ref_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard = pd.concat([summary_table, df_ref_categories])\n",
    "# Concatenates two dataframes.\n",
    "df_scorecard = df_scorecard.reset_index()\n",
    "# We reset the index of a dataframe.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]\n",
    "# We create a new column, called 'Original feature name', which contains the value of the 'Feature name' column,\n",
    "# up to the column symbol.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = 300\n",
    "max_score = 850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard.groupby('Original feature name')['Coefficient'].min()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficient'].min().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "# Then, we aggregate further and sum all the minimum values.\n",
    "min_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_scorecard.groupby('Original feature name')['Coefficient'].max()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficient'].max().sum()\n",
    "# Up to the 'min()' method everything is the same as in te line above.\n",
    "# Then, we aggregate further and sum all the maximum values.\n",
    "max_sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'] = df_scorecard['Coefficient'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)\n",
    "# We multiply the value of the 'Coefficients' column by the ration of the differences between\n",
    "# maximum score and minimum score and maximum sum of coefficients and minimum sum of cefficients.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Calculation'][0] = ((df_scorecard['Coefficient'][0] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score\n",
    "# We divide the difference of the value of the 'Coefficients' column and the minimum sum of coefficients by\n",
    "# the difference of the maximum sum of coefficients and the minimum sum of coefficients.\n",
    "# Then, we multiply that by the difference between the maximum score and the minimum score.\n",
    "# Then, we add minimum score. \n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()\n",
    "# We round the values of the 'Score - Calculation' column.\n",
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their minimum.\n",
    "# Sums all minimum values.\n",
    "min_sum_score_prel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()\n",
    "# Groups the data by the values of the 'Original feature name' column.\n",
    "# Aggregates the data in the 'Coefficients' column, calculating their maximum.\n",
    "# Sums all maximum values.\n",
    "max_sum_score_prel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caclulating Credit Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_inputs_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept = loan_data_inputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n",
    "# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n",
    "# The name of that column is 'Intercept', and its values are 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept = inputs_test_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n",
    "# Here, from the 'inputs_test_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n",
    "# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores = df_scorecard['Score - Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test_with_ref_cat_w_intercept.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores = scorecard_scores.values.reshape(64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorecard_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = inputs_test_with_ref_cat_w_intercept.dot(scorecard_scores)\n",
    "# Here we multiply the values of each row of the dataframe by the values of each column of the variable,\n",
    "# which is an argument of the 'dot' method, and sum them. It's essentially the sum of the products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Credit Score to PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_coef_from_score = ((y_scores - min_score) / (max_score - min_score)) * (max_sum_coef - min_sum_coef) + min_sum_coef\n",
    "# We divide the difference between the scores and the minimum score by\n",
    "# the difference between the maximum score and the minimum score.\n",
    "# Then, we multiply that by the difference between the maximum sum of coefficients and the minimum sum of coefficients.\n",
    "# Then, we add the minimum sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_proba_from_score = np.exp(sum_coef_from_score) / (np.exp(sum_coef_from_score) + 1)\n",
    "# Here we divide an exponent raised to sum of coefficients from score by\n",
    "# an exponent raised to sum of coefficients from score plus one.\n",
    "y_hat_proba_from_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_proba[0: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_predicted_probs['y_hat_test_proba'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Cut-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the confusion matrix again.\n",
    "#np.where(np.squeeze(np.array(loan_data_targets_test)) == np.where(y_hat_test_proba >= tr, 1, 0), 1, 0).sum() / loan_data_targets_test.shape[0]\n",
    "tr = 0.8\n",
    "df_actual_predicted_probs['y_hat_test'] = np.where(df_actual_predicted_probs['y_hat_test_proba'] > tr, 1, 0)\n",
    "#df_actual_predicted_probs['loan_data_targets_test'] == np.where(df_actual_predicted_probs['y_hat_test_proba'] >= tr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test'], rownames = ['Actual'], colnames = ['Predicted']) / df_actual_predicted_probs.shape[0]).iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['loan_data_targets_test'], df_actual_predicted_probs['y_hat_test_proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs = pd.concat([pd.DataFrame(thresholds), pd.DataFrame(fpr), pd.DataFrame(tpr)], axis = 1)\n",
    "# We concatenate 3 dataframes along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.columns = ['thresholds', 'fpr', 'tpr']\n",
    "# We name the columns of the dataframe 'thresholds', 'fpr', and 'tpr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['thresholds'][0] = 1 - 1 / np.power(10, 16)\n",
    "# Let the first threshold (the value of the thresholds column with index 0) be equal to a number, very close to 1\n",
    "# but smaller than 1, say 1 - 1 / 10 ^ 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] / (1 - df_cutoffs['thresholds'])) - min_sum_coef) * ((max_score - min_score) / (max_sum_coef - min_sum_coef)) + min_score).round()\n",
    "# The score corresponsing to each threshold equals:\n",
    "# The the difference between the natural logarithm of the ratio of the threshold and 1 minus the threshold and\n",
    "# the minimum sum of coefficients\n",
    "# multiplied by\n",
    "# the sum of the minimum score and the ratio of the difference between the maximum score and minimum score and \n",
    "# the difference between the maximum sum of coefficients and the minimum sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['Score'][0] = max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function called 'n_approved' which assigns a value of 1 if a predicted probability\n",
    "# is greater than the parameter p, which is a threshold, and a value of 0, if it is not.\n",
    "# Then it sums the column.\n",
    "# Thus, if given any percentage values, the function will return\n",
    "# the number of rows wih estimated probabilites greater than the threshold. \n",
    "def n_approved(p):\n",
    "    return np.where(df_actual_predicted_probs['y_hat_test_proba'] >= p, 1, 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)\n",
    "# Assuming that all credit applications above a given probability of being 'good' will be approved,\n",
    "# when we apply the 'n_approved' function to a threshold, it will return the number of approved applications.\n",
    "# Thus, here we calculate the number of approved appliations for al thresholds.\n",
    "df_cutoffs['N Rejected'] = df_actual_predicted_probs['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']\n",
    "# Then, we calculate the number of rejected applications for each threshold.\n",
    "# It is the difference between the total number of applications and the approved applications for that threshold.\n",
    "df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / df_actual_predicted_probs['y_hat_test_proba'].shape[0]\n",
    "# Approval rate equalts the ratio of the approved applications and all applications.\n",
    "df_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']\n",
    "# Rejection rate equals one minus approval rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cutoffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
